# Coin87 Data Ingestion Technical Review
## Giai ƒëo·∫°n 1: Thu th·∫≠p d·ªØ li·ªáu (Data Ingestion)

**Reviewer:** Senior Data Engineer & Solution Architect  
**Date:** January 29, 2026  
**Status:** ‚úÖ Completed with Recommendations

---

## Executive Summary

D·ª± √°n Coin87 ƒëang x√¢y d·ª±ng n·ªÅn t·∫£ng t·ªïng h·ª£p tin t·ª©c v√† ph√¢n t√≠ch Crypto v·ªõi m·ª•c ti√™u ∆∞u ti√™n **ƒë·ªô bao ph·ªß (coverage)** thay v√¨ ƒë·ªô tr·ªÖ th·∫•p. Ki·∫øn tr√∫c hi·ªán t·∫°i s·ª≠ d·ª•ng Python/Node.js v·ªõi PostgreSQL v√† Redis, t·∫≠p trung v√†o ngu·ªìn d·ªØ li·ªáu mi·ªÖn ph√≠ v√† kh√¥ng ch·∫•p nh·∫≠n API tr·∫£ ph√≠.

### ƒê√°nh gi√° t·ªïng quan
- ‚úÖ **Ki·∫øn tr√∫c c∆° b·∫£n:** V·ªØng ch·∫Øc, ph√π h·ª£p v·ªõi m·ª•c ti√™u MVP
- ‚ö†Ô∏è **·ªîn ƒë·ªãnh:** C·∫ßn b·ªï sung WebSocket reconnection v√† circuit breaker
- ‚ö†Ô∏è **Rate Limiting:** Thi·∫øu adaptive backoff, r·ªßi ro b·ªã ch·∫∑n IP cao
- ‚úÖ **Schema:** Ph√¢n t√°ch r√µ r√†ng raw/derived, h·ªó tr·ª£ immutability
- ‚ö†Ô∏è **Blind Spots:** Thi·∫øu on-chain data, sentiment analysis, v√† social signals ƒëa d·∫°ng

---

## 1. T√≠nh ·ªïn ƒë·ªãnh (Stability) ‚≠ê‚≠ê‚≠ê (3/5)

### ‚úÖ ƒêi·ªÉm m·∫°nh

1. **Partial Success Semantics**
   - Ingestion job x·ª≠ l√Ω l·ªói t·ª´ng source ƒë·ªôc l·∫≠p
   - Kh√¥ng crash to√†n b·ªô pipeline khi 1 source fail
   - Code: [run_ingestion.py](backend/ingestion/jobs/run_ingestion.py#L78-L110)

2. **Deduplication Strategy**
   - Content hash SHA256 (abstract + source_name)
   - INSERT ... ON CONFLICT DO NOTHING t·∫°i database layer
   - Savepoint isolation cho t·ª´ng insert
   - Code: [rss_adapter.py](backend/ingestion/adapters/rss_adapter.py#L174-L195)

3. **Conservative Fetch Behavior**
   - Jitter (1-5s random delay) tr√°nh bot pattern
   - Per-source rate limiting
   - User-Agent rotation
   - Code: [fetch_context.py](backend/ingestion/core/fetch_context.py#L45-L85)

### ‚ùå ƒêi·ªÉm y·∫øu

1. **Thi·∫øu WebSocket Reconnection Logic**
   - **V·∫•n ƒë·ªÅ:** K·∫ø ho·∫°ch s·ª≠ d·ª•ng WebSocket cho Binance/Bybit nh∆∞ng ch∆∞a c√≥ implementation
   - **R·ªßi ro:** Khi WebSocket disconnect (network hiccup, server restart), m·∫•t d·ªØ li·ªáu real-time
   - **ƒê·ªÅ xu·∫•t:** Implement WebSocket client v·ªõi:
     - Auto-reconnect v·ªõi exponential backoff
     - State recovery t·ª´ last sequence number
     - Heartbeat/ping-pong ƒë·ªÉ detect stale connections
   - **Th∆∞ vi·ªán ƒë·ªÅ xu·∫•t:**
     ```python
     # Python
     import websockets  # Async WebSocket client
     import backoff     # Exponential backoff decorator
     
     @backoff.on_exception(backoff.expo, websockets.ConnectionClosed, max_tries=10)
     async def connect_binance_ws():
         async with websockets.connect("wss://stream.binance.com:9443/ws/btcusdt@trade") as ws:
             # Handle messages with state recovery
             pass
     ```

2. **Kh√¥ng c√≥ Circuit Breaker cho External APIs**
   - **V·∫•n ƒë·ªÅ:** [ingestion_controller.py](backend/ingestion/core/ingestion_controller.py) c√≥ circuit breaker nh∆∞ng ch∆∞a t√≠ch h·ª£p v√†o Job A
   - **R·ªßi ro:** Li√™n t·ª•c g·ªçi API ƒë√£ fail ‚Üí waste resources, tƒÉng risk b·ªã rate limit
   - **ƒê·ªÅ xu·∫•t:** T√≠ch h·ª£p circuit breaker v√†o FetchContext:
     ```python
     # Th√™m v√†o FetchContext
     if circuit_breaker.is_open(source.key):
         return None, None, {"skipped": True, "reason": "circuit_open"}
     ```

3. **Thi·∫øu Health Monitoring Dashboard**
   - **V·∫•n ƒë·ªÅ:** Kh√¥ng c√≥ visibility v√†o ingestion health real-time
   - **ƒê·ªÅ xu·∫•t:** Expose metrics qua Prometheus/Grafana:
     - Source availability percentage
     - Average response time
     - Error rate by source
     - Dedup rate (indicates stale data)

### üîß C·∫£i thi·ªán ƒë√£ implement

‚úÖ **Adaptive Backoff Strategy** (COMPLETED)
- Added `SourceHealth` tracking cho m·ªói source
- Exponential backoff: 1min ‚Üí 2min ‚Üí 4min ‚Üí ... ‚Üí 1 hour (capped)
- Sticky proxy rotation (ch·ªâ ƒë·ªïi proxy khi fail)
- Code: [fetch_context.py](backend/ingestion/core/fetch_context.py#L33-L66)

### üìä Kh·∫£ nƒÉng ch·ªãu t·∫£i (Load Capacity)

| Scenario | Current Capacity | Risk Level |
|----------|------------------|------------|
| Normal market (50 sources √ó 20 min/poll) | ‚úÖ ~150 requests/hour | Low |
| High volatility (burst traffic) | ‚ö†Ô∏è No throttling | **High** |
| Multiple 403/429 errors | ‚úÖ Now has backoff | Low |
| WebSocket disconnect | ‚ùå Not implemented | **Critical** |

**K·∫øt lu·∫≠n:** 
- ‚úÖ ƒê·ªß ·ªïn ƒë·ªãnh cho batch ingestion (RSS, Reddit)
- ‚ùå Ch∆∞a s·∫µn s√†ng cho real-time WebSocket (c·∫ßn implement tr∆∞·ªõc khi enable)

---

## 2. Qu·∫£n l√Ω gi·ªõi h·∫°n (Rate Limiting & Proxy) ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)

### ‚úÖ ƒêi·ªÉm m·∫°nh

1. **Per-Source Rate Limiting**
   - Configurable qua `sources.yaml`
   - ƒê√£ tƒÉng t·ª´ 15s ‚Üí 20-120s (ph√π h·ª£p v·ªõi free tier)
   - Code: [sources.yaml](backend/ingestion/config/sources.yaml)

2. **Proxy Support Infrastructure**
   - Environment variable `C87_PROXY_URL` h·ªó tr·ª£ nhi·ªÅu proxy (comma-separated)
   - Sticky proxy strategy (gi·ªØ proxy cho source cho ƒë·∫øn khi fail)
   - Code: [fetch_context.py](backend/ingestion/core/fetch_context.py#L125-L145)

3. **Conservative User Agents**
   - Kh√¥ng d√πng headless browser UA (tr√°nh b·ªã detect)
   - Random rotation gi·ªØa real browser UAs
   - Code: [fetch_context.py](backend/ingestion/core/fetch_context.py#L22-L26)

### ‚ö†Ô∏è R·ªßi ro b·ªã ch·∫∑n IP

| Source Type | Risk Level | Mitigation Strategy |
|-------------|------------|---------------------|
| **RSS Feeds** | üü¢ Low | Standard politeness (20s interval) |
| **CoinGecko API** | üü° Medium | Free tier: 10-50 calls/min. **Use rate_limit_seconds=120** |
| **Twitter/X Scraping** | üî¥ **High** | ‚ùå Kh√¥ng kh·∫£ thi v·ªõi free tier. **ƒê·ªÅ xu·∫•t:** RSS c·ªßa crypto influencers thay th·∫ø |
| **Reddit API (PRAW)** | üü¢ Low | Free tier: 60 req/min. **Hi·ªán t·∫°i: 120s interval = an to√†n** |
| **Telegram (Telethon)** | üü° Medium | C·∫ßn phone number authentication. **C·∫©n th·∫≠n v·ªõi flood wait** |
| **Google News Scraping** | üî¥ **High** | Anti-bot protection m·∫°nh. **ƒê·ªÅ xu·∫•t:** D√πng RSS thay th·∫ø |

### üîß Chi·∫øn l∆∞·ª£c Proxy

#### Option A: Free Proxy Rotation (NOT RECOMMENDED)
```bash
# Public proxies - th∆∞·ªùng b·ªã ch·∫∑n, kh√¥ng stable
C87_PROXY_URL=http://proxy1.free.com:8080,http://proxy2.free.com:8080
```
**V·∫•n ƒë·ªÅ:**
- Free proxies th∆∞·ªùng ƒë√£ b·ªã websites blacklist
- Uptime th·∫•p (< 80%)
- R·ªßi ro security (MITM attacks)

#### Option B: Residential Proxy Services (PAID - Khuy·∫øn ngh·ªã n·∫øu scale)
```bash
# Rotating residential proxies (n·∫øu c·∫ßn trong t∆∞∆°ng lai)
# Bright Data: ~$500/month (40GB)
# Oxylabs: ~$300/month (20GB)
# SmartProxy: ~$75/month (5GB)
```

#### Option C: D√πng VPS ·ªü nhi·ªÅu region (RECOMMENDED cho MVP) ‚≠ê
```bash
# Deploy 3-5 VPS ·ªü c√°c regions kh√°c nhau
# DigitalOcean/Vultr: $5/month √ó 3 = $15/month
C87_PROXY_URL=http://vps1.singapore.com:3128,http://vps2.frankfurt.com:3128,http://vps3.nyc.com:3128
```
**∆Øu ƒëi·ªÉm:**
- IP s·∫°ch, √≠t b·ªã blacklist
- Control to√†n b·ªô infrastructure
- R·∫ª h∆°n residential proxies
- C√≥ th·ªÉ c√†i Squid proxy ho·∫∑c Tinyproxy

### üîß C·∫£i thi·ªán ƒë√£ implement

‚úÖ **Sticky Proxy Strategy** (COMPLETED)
- Kh√¥ng ƒë·ªïi proxy li√™n t·ª•c (gi·∫£m fingerprint thay ƒë·ªïi)
- Ch·ªâ rotate khi g·∫∑p 403/429
- Code: [fetch_context.py](backend/ingestion/core/fetch_context.py#L125-L145)

‚úÖ **Adaptive Backoff on 403/429** (COMPLETED)
- T·ª± ƒë·ªông tƒÉng delay khi b·ªã rate limit
- Track `total_403_429` ƒë·ªÉ trigger proxy rotation
- Code: [fetch_context.py](backend/ingestion/core/fetch_context.py#L48-L66)

### üìã API Usage Strategy (Free Tier Only)

| Service | Free Limit | Our Strategy | Status |
|---------|-----------|--------------|--------|
| **Reddit API** | 60 req/min | 1 req/120s √ó 4 subreddits = **safe** | ‚úÖ Implemented |
| **CoinGecko** | 10-50 calls/min | 1 req/120s = **safe** | ‚¨ú TODO |
| **RSS Feeds** | Unlimited (v·ªõi politeness) | 1 req/20-30s = **safe** | ‚úÖ Implemented |
| **GitHub Releases** | 5000 req/hour | 1 req/120s = **safe** | ‚úÖ Implemented |
| **Telegram** | ~20 req/min (flood wait risk) | 1 req/60s = **safe** | ‚¨ú TODO |

### üö® Ngu·ªìn d·ªØ li·ªáu KH√îNG kh·∫£ thi (c·∫ßn b·ªè ho·∫∑c t√¨m thay th·∫ø)

‚ùå **Twitter/X Direct Scraping**
- **V·∫•n ƒë·ªÅ:** Rate limit c·ª±c k·ª≥ nghi√™m ng·∫∑t, c·∫ßn login
- **Thay th·∫ø:** 
  - RSS feeds c·ªßa crypto influencers (nitter.net instances)
  - Nitter RSS: `https://nitter.net/{username}/rss`
  - V√≠ d·ª•: `https://nitter.net/VitalikButerin/rss`

‚ùå **Google News Scraping**
- **V·∫•n ƒë·ªÅ:** Anti-bot protection m·∫°nh (CAPTCHA, JS rendering required)
- **Thay th·∫ø:**
  - Google News RSS (public): `https://news.google.com/rss/search?q=bitcoin&hl=en`
  - Bing News RSS: `https://www.bing.com/news/search?q=cryptocurrency&format=rss`

### üéØ ƒê·ªÅ xu·∫•t c·∫£i thi·ªán

1. **Implement Rate Limit Retry-After Header Respect**
   ```python
   # Trong fetch_context.py
   if resp.status_code == 429:
       retry_after = resp.headers.get("Retry-After", 60)
       health.backoff_until_epoch = time.time() + int(retry_after)
   ```

2. **Add IP Rotation Test Script**
   ```python
   # scripts/test_proxy_rotation.py
   # Test t·∫•t c·∫£ proxies tr∆∞·ªõc khi ch·∫°y ingestion
   # Verify IP kh√¥ng b·ªã blacklist b·ªüi target sites
   ```

3. **Monitor Rate Limit Violations**
   ```python
   # Log metrics to PostgreSQL/TimescaleDB
   INSERT INTO rate_limit_events (timestamp, source, status_code, retry_after)
   VALUES (NOW(), 'coindesk_rss', 429, 120);
   ```

---

## 3. Ki·∫øn tr√∫c d·ªØ li·ªáu (Data Schema) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

### ‚úÖ ƒêi·ªÉm m·∫°nh (Xu·∫•t s·∫Øc)

1. **Ph√¢n t√°ch Raw vs Derived r√µ r√†ng**
   ```
   information_events (raw)     ‚Üí Immutable, append-only
        ‚Üì
   narratives (derived)         ‚Üí Clustering/grouping
        ‚Üì
   risk_events (derived)        ‚Üí Risk classification
        ‚Üì
   environment_snapshots        ‚Üí Time-series snapshots
   ```
   - **L·ª£i √≠ch:** D·ªÖ audit, rollback, reprocess data
   - Code: [models/information_event.py](backend/app/models/information_event.py)

2. **Immutability Enforcement**
   - `information_events` kh√¥ng c√≥ UPDATE operations
   - Content hash SHA256 cho deduplication
   - `observed_at` vs `event_time` t√°ch r·ªùi
   - **Test coverage:** [test_immutability.py](backend/tests/test_immutability.py)

3. **Metadata Storage (JSONB)**
   - `raw_payload` JSONB cho flexibility
   - Kh√¥ng m·∫•t th√¥ng tin g·ªëc khi normalize
   - Query ƒë∆∞·ª£c v·ªõi GIN index

### üìä Schema Split Evaluation

| Data Type | Current DB | Optimal DB | Reason |
|-----------|-----------|------------|--------|
| `information_events` | PostgreSQL | ‚úÖ PostgreSQL | Immutability, ACID compliance |
| `narratives` | PostgreSQL | ‚úÖ PostgreSQL | Complex joins, clustering logic |
| `risk_events` | PostgreSQL | ‚úÖ PostgreSQL | Governance, audit trail |
| `governance_logs` | PostgreSQL | ‚úÖ PostgreSQL | Compliance, immutability |
| **Price ticks (future)** | PostgreSQL | ‚ö†Ô∏è **TimescaleDB** | High write volume (10k+/sec) |
| **Sentiment scores (future)** | PostgreSQL | ‚ö†Ô∏è **TimescaleDB** | Time-series aggregation |
| **Source health metrics** | PostgreSQL | ‚ö†Ô∏è **TimescaleDB** | Retention policies needed |
| **Cache/Sessions** | Redis | ‚úÖ Redis | Ephemeral, fast access |
| **Rate limiting** | Redis | ‚úÖ Redis | In-memory counters |

### üéØ ƒê·ªÅ xu·∫•t Time-Series Migration

**K·ªãch b·∫£n khi n√†o c·∫ßn migrate:**
- Khi th√™m WebSocket ingestion (price ticks > 1000/sec)
- Khi l∆∞u sentiment scores theo ph√∫t
- Khi dashboard c·∫ßn real-time metrics

**Solution: TimescaleDB Extension** (Documented in [TIMESERIES_MIGRATION.md](TIMESERIES_MIGRATION.md))

**∆Øu ƒëi·ªÉm:**
- ‚úÖ PostgreSQL extension (kh√¥ng c·∫ßn separate service)
- ‚úÖ SQL interface (team ƒë√£ familiar)
- ‚úÖ Free, open-source
- ‚úÖ Automatic partitioning, compression, retention
- ‚úÖ 10x better performance cho time-series queries

**K·∫øt lu·∫≠n:** 
- ‚úÖ **Schema hi·ªán t·∫°i: Ho√†n h·∫£o cho batch ingestion**
- ‚¨ú **TimescaleDB: Enable khi c·∫ßn real-time metrics**

---

## 4. ƒêi·ªÉm m√π (Blind Spots) ‚≠ê‚≠ê‚≠ê (3/5)

### ‚ùå Ngu·ªìn d·ªØ li·ªáu quan tr·ªçng ƒëang thi·∫øu

#### 1. On-Chain Data (CRITICAL) üî¥

**Hi·ªán t·∫°i:** Ch∆∞a c√≥ implementation  
**T·∫ßm quan tr·ªçng:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Critical cho ph√¢n t√≠ch th·ªã tr∆∞·ªùng)

**Ngu·ªìn mi·ªÖn ph√≠:**

```yaml
# Th√™m v√†o sources.yaml

# Whale Alert (Twitter/RSS)
whalealert_twitter_rss:
  enabled: true
  type: rss
  name: Whale Alert RSS
  url: https://nitter.net/whale_alert/rss
  rate_limit_seconds: 60
  proxy: false
  priority: high

# Etherscan Gas Tracker (RSS)
etherscan_gas_rss:
  enabled: true
  type: rss
  name: Etherscan Gas Tracker
  url: https://etherscan.io/gastracker/rss
  rate_limit_seconds: 120
  proxy: false
  priority: medium

# Blockchain.com Unconfirmed Transactions
blockchain_mempool_api:
  enabled: false  # C·∫ßn adapter ri√™ng
  type: api
  name: Blockchain.com Mempool
  url: https://blockchain.info/unconfirmed-transactions?format=json
  rate_limit_seconds: 300
  proxy: false
  priority: low
```

**Free APIs for On-Chain:**
- **Etherscan:** 5 calls/sec (free), 100k calls/day
- **BscScan:** Similar limits
- **Blockchain.com:** Unlimited for basic endpoints
- **Blockchair:** 1 req/1.5sec (free tier)

**ƒê·ªÅ xu·∫•t implementation:**
```python
# backend/ingestion/adapters/onchain_adapter.py
class OnChainAdapter(BaseAdapter):
    """Fetch on-chain metrics from free APIs."""
    
    def fetch_whale_movements(self, min_value_usd=1_000_000):
        # Etherscan: Get large transactions
        pass
    
    def fetch_gas_prices(self):
        # Current gas prices + trend
        pass
    
    def fetch_exchange_flows(self):
        # Net inflow/outflow to exchanges
        pass
```

#### 2. Sentiment Analysis (MEDIUM PRIORITY) üü°

**Hi·ªán t·∫°i:** Ch·ªâ c√≥ raw text, ch∆∞a extract sentiment  
**T·∫ßm quan tr·ªçng:** ‚≠ê‚≠ê‚≠ê‚≠ê

**Free Solutions:**
```python
# Option 1: TextBlob (simple, fast)
from textblob import TextBlob

def analyze_sentiment(text: str) -> float:
    blob = TextBlob(text)
    return blob.sentiment.polarity  # -1 to 1

# Option 2: VADER (crypto-optimized lexicon)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()
scores = analyzer.polarity_scores(text)
# {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.8}

# Option 3: FinBERT (best accuracy, slower)
from transformers import AutoTokenizer, AutoModelForSequenceClassification
# Hugging Face: ProsusAI/finbert
```

**ƒê·ªÅ xu·∫•t:** 
- B·∫Øt ƒë·∫ßu v·ªõi VADER (crypto-friendly)
- Upgrade l√™n FinBERT khi c·∫ßn ƒë·ªô ch√≠nh x√°c cao

#### 3. Market Data Aggregation (HIGH PRIORITY) üî¥

**Hi·ªán t·∫°i:** Ch·ªâ c√≥ tin t·ª©c, ch∆∞a c√≥ gi√°/volume  
**T·∫ßm quan tr·ªçng:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Free APIs:**

```yaml
# CoinGecko (Free Tier)
coingecko_prices:
  enabled: true
  type: api
  name: CoinGecko Price Feed
  url: https://api.coingecko.com/api/v3/simple/price?ids=bitcoin,ethereum&vs_currencies=usd&include_24hr_change=true
  rate_limit_seconds: 120  # Free: 10-50 calls/min
  proxy: false
  priority: high

# CryptoCompare (Free Tier)
cryptocompare_ohlcv:
  enabled: false
  type: api
  name: CryptoCompare OHLCV
  url: https://min-api.cryptocompare.com/data/v2/histominute
  rate_limit_seconds: 60  # Free: 100k calls/month
  proxy: false
  priority: medium

# Binance Public API (No auth needed)
binance_ticker:
  enabled: false
  type: api
  name: Binance 24h Ticker
  url: https://api.binance.com/api/v3/ticker/24hr
  rate_limit_seconds: 10  # 1200 weight/min
  proxy: false
  priority: high
```

**Adapter Example:**
```python
# backend/ingestion/adapters/market_data_adapter.py
class MarketDataAdapter(BaseAdapter):
    def fetch_coingecko_prices(self, coin_ids: list[str]):
        # Batch fetch prices for multiple coins
        pass
    
    def fetch_binance_ticker(self, symbol: str):
        # Get 24h price change, volume
        pass
```

#### 4. DeFi Protocol Data (MEDIUM PRIORITY) üü°

**Hi·ªán t·∫°i:** Kh√¥ng c√≥  
**T·∫ßm quan tr·ªçng:** ‚≠ê‚≠ê‚≠ê

**Free Sources:**
- **DefiLlama API:** Free, unlimited (TVL, yields)
- **Uniswap Subgraph:** Free GraphQL (volume, liquidity)
- **AAVE API:** Free (borrow/lend rates)

```yaml
defillama_tvl:
  enabled: false
  type: api
  name: DefiLlama TVL
  url: https://api.llama.fi/protocols
  rate_limit_seconds: 300
  proxy: false
  priority: low
```

### ‚úÖ Ngu·ªìn d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c b·ªï sung

1. **Reddit Integration** ‚úÖ (COMPLETED)
   - 4 major subreddits (r/cryptocurrency, r/bitcoin, r/ethereum, r/cryptomarkets)
   - PRAW adapter v·ªõi free API
   - Code: [reddit_adapter.py](backend/ingestion/adapters/reddit_adapter.py)

2. **Expanded RSS Feeds** ‚úÖ (COMPLETED)
   - 9 major crypto news sites (t·ª´ 2 ‚Üí 9 sources)
   - GitHub releases cho Bitcoin/Ethereum
   - Code: [sources.yaml](backend/ingestion/config/sources.yaml)

### üéØ Roadmap b·ªï sung ngu·ªìn d·ªØ li·ªáu

| Priority | Data Source | Effort | Value | Implementation Timeline |
|----------|-------------|--------|-------|-------------------------|
| üî¥ P0 | On-Chain (Whale movements) | 3 days | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Week 2 |
| üî¥ P0 | Market Data (CoinGecko) | 2 days | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Week 2 |
| üü° P1 | Sentiment Analysis (VADER) | 1 day | ‚≠ê‚≠ê‚≠ê‚≠ê | Week 3 |
| üü° P1 | Telegram Channels | 5 days | ‚≠ê‚≠ê‚≠ê‚≠ê | Week 4 |
| üü¢ P2 | DeFi Protocols (DefiLlama) | 2 days | ‚≠ê‚≠ê‚≠ê | Week 5 |
| üü¢ P2 | NFT Markets (OpenSea API) | 3 days | ‚≠ê‚≠ê | Backlog |

---

## 5. Tech Stack Evaluation

### Current Stack Assessment

| Component | Technology | Grade | Notes |
|-----------|-----------|-------|-------|
| **Crawler/ETL** | Python | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Excellent choice, rich ecosystem |
| **WebSocket** | Node.js (planned) | ‚≠ê‚≠ê‚≠ê | OK, but Python async better |
| **Database** | PostgreSQL | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Perfect for relational data |
| **Cache** | Redis | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Industry standard |
| **Queue** | Redis (current) | ‚≠ê‚≠ê‚≠ê | OK for MVP, migrate to RabbitMQ later |
| **Time-Series** | None | ‚≠ê | **Missing**, need TimescaleDB |

### üîß ƒê·ªÅ xu·∫•t c·∫£i thi·ªán Tech Stack

#### 1. WebSocket: Python async thay v√¨ Node.js ‚≠ê

**L√Ω do:**
- Python c√≥ `websockets`, `aiohttp` r·∫•t m·∫°nh
- Gi·ªØ codebase ƒë·ªìng nh·∫•t (all Python)
- Easier deployment (1 runtime thay v√¨ 2)

```python
# backend/ingestion/adapters/binance_ws_adapter.py
import asyncio
import websockets
import backoff

class BinanceWebSocketAdapter(BaseAdapter):
    @backoff.on_exception(backoff.expo, websockets.ConnectionClosed, max_tries=10)
    async def connect(self, symbol: str):
        uri = f"wss://stream.binance.com:9443/ws/{symbol.lower()}@trade"
        async with websockets.connect(uri) as ws:
            async for message in ws:
                await self.process_message(message)
```

#### 2. Message Queue: Redis ‚Üí Celery + RabbitMQ (future)

**Hi·ªán t·∫°i:** Redis Streams OK cho MVP  
**Khi n√†o c·∫ßn migrate:** Khi c√≥ > 10k tasks/hour

**∆Øu ƒëi·ªÉm RabbitMQ:**
- Better durability (persistent queues)
- Retry logic built-in
- Dead letter queues
- Better monitoring (RabbitMQ Management UI)

#### 3. Monitoring Stack: Th√™m Prometheus + Grafana

**Essential metrics:**
```python
# backend/ingestion/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge

ingestion_total = Counter('coin87_ingestion_total', 'Total ingestion attempts', ['source', 'status'])
ingestion_duration = Histogram('coin87_ingestion_duration_seconds', 'Ingestion duration', ['source'])
source_health = Gauge('coin87_source_health', 'Source health score', ['source'])
```

---

## 6. Security & Compliance

### ‚úÖ Current Security Posture

1. **API Keys Management**
   - ‚úÖ Environment variables (kh√¥ng hardcode)
   - ‚úÖ `.env` trong `.gitignore`
   - Code: [.env](d:\projects\coin87\coin87Project\.env)

2. **Read-Only Operations**
   - ‚úÖ Ingestion ch·ªâ INSERT, kh√¥ng UPDATE/DELETE
   - ‚úÖ Test coverage cho immutability
   - Code: [test_immutability.py](backend/tests/test_immutability.py)

3. **Input Validation**
   - ‚úÖ Pydantic schemas cho API endpoints
   - ‚úÖ Content sanitization (strip HTML/markdown)
   - Code: [rss_adapter.py](backend/ingestion/adapters/rss_adapter.py#L34-L36)

### ‚ö†Ô∏è Security Gaps

1. **Proxy Credentials Exposure**
   ```bash
   # ‚ùå BAD: Credentials in URL
   C87_PROXY_URL=http://user:pass@proxy.com:8080
   
   # ‚úÖ BETTER: Separate credentials
   C87_PROXY_URL=http://proxy.com:8080
   C87_PROXY_USER=user
   C87_PROXY_PASS=pass  # Still in env, but separated
   
   # ‚úÖ BEST: Use secrets management
   # AWS Secrets Manager, HashiCorp Vault, etc.
   ```

2. **Rate Limit Bypass Detection**
   - Implement logging cho suspicious patterns:
     - Qu√° nhi·ªÅu 403/429 t·ª´ c√πng 1 source
     - Proxy rotation qu√° nhanh
     - IP blacklist detection

3. **Data Retention Policy**
   - C·∫ßn policy cho GDPR compliance (n·∫øu c√≥ EU users)
   - Anonymize/delete user data after retention period
   - Implement trong TimescaleDB retention policies

---

## 7. Performance Optimization

### Current Bottlenecks

1. **Sequential Source Processing**
   ```python
   # Current: process sources one by one
   for source in registry.enabled_sources():
       fetch_and_insert(source)  # Blocking
   ```
   
   **ƒê·ªÅ xu·∫•t:**
   ```python
   # Async parallel processing (limit concurrency)
   import asyncio
   from asyncio import Semaphore
   
   async def process_sources_parallel(sources, max_concurrent=5):
       semaphore = Semaphore(max_concurrent)
       tasks = [fetch_with_semaphore(source, semaphore) for source in sources]
       await asyncio.gather(*tasks)
   ```

2. **Database Connection Pooling**
   ```python
   # backend/app/core/db.py
   from sqlalchemy import create_engine
   
   engine = create_engine(
       DATABASE_URL,
       pool_size=10,          # Increase from default 5
       max_overflow=20,       # Allow burst connections
       pool_pre_ping=True,    # Verify connections before use
       pool_recycle=3600,     # Recycle connections every hour
   )
   ```

3. **Redis Pipelining for Bulk Inserts**
   ```python
   # Instead of individual SET commands
   pipe = redis_client.pipeline()
   for item in items:
       pipe.set(f"cache:{item.id}", item.data)
   pipe.execute()  # Single round-trip
   ```

### Performance Targets

| Metric | Current | Target | Method |
|--------|---------|--------|--------|
| Ingestion latency | ~30s/source | ~10s/source | Async parallel |
| DB write throughput | ~100 rows/min | ~1000 rows/min | Batch inserts |
| API response time | ~200ms | ~50ms | Redis cache |
| Source health check | Manual | <5s | Automated dashboard |

---

## 8. Deployment & DevOps

### Recommended Deployment Strategy

```yaml
# docker-compose.yml (production)
version: '3.8'
services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      POSTGRES_DB: coin87_db
      POSTGRES_USER: coin87_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: always

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    restart: always

  ingestion_worker:
    build: ./backend
    command: python ingestion/jobs/run_ingestion.py
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDDIT_CLIENT_ID=${REDDIT_CLIENT_ID}
      - C87_PROXY_URL=${C87_PROXY_URL}
    depends_on:
      - postgres
      - redis
    restart: on-failure

  api_server:
    build: ./backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    restart: always

volumes:
  postgres_data:
  redis_data:
```

### Monitoring Setup

```yaml
# docker-compose.monitoring.yml
services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
```

---

## 9. Cost Estimation

### Monthly Infrastructure Cost (MVP)

| Component | Service | Cost |
|-----------|---------|------|
| **Database** | PostgreSQL (DigitalOcean 2GB) | $15 |
| **Redis** | Redis Cloud (500MB) | $0 (free tier) |
| **Application Server** | VPS (2 vCPU, 4GB RAM) | $12 |
| **Proxy VPS (optional)** | 3√ó VPS (1GB RAM) | $15 |
| **Domain + SSL** | Cloudflare | $0 (free tier) |
| **Monitoring** | Grafana Cloud (free tier) | $0 |
| **Total** | | **$42/month** |

### Cost at Scale (1000 sources, 10k users)

| Component | Service | Cost |
|-----------|---------|------|
| Database | PostgreSQL (8GB + TimescaleDB) | $60 |
| Redis | Redis Cloud (2GB) | $25 |
| Application | 2√ó Load-balanced VPS | $50 |
| Proxies | Rotating residential (optional) | $75 |
| CDN | Cloudflare Pro | $20 |
| Monitoring | Grafana Cloud Standard | $0 (still free) |
| **Total** | | **$230/month** |

---

## 10. Action Items & Roadmap

### üî¥ Critical (Do Now)

- [x] ‚úÖ Implement Reddit adapter
- [x] ‚úÖ Add adaptive backoff strategy
- [x] ‚úÖ Expand RSS source coverage
- [x] ‚úÖ Add TimescaleDB migration plan
- [ ] ‚¨ú Add CoinGecko price adapter (2 days)
- [ ] ‚¨ú Add on-chain data adapter (3 days)
- [ ] ‚¨ú Implement WebSocket reconnection logic (3 days)

### üü° High Priority (This Month)

- [ ] ‚¨ú Add sentiment analysis (VADER) (1 day)
- [ ] ‚¨ú Setup Prometheus + Grafana monitoring (2 days)
- [ ] ‚¨ú Implement async parallel source processing (2 days)
- [ ] ‚¨ú Add health check dashboard (1 day)
- [ ] ‚¨ú Write deployment scripts (Docker Compose) (1 day)

### üü¢ Medium Priority (Next Quarter)

- [ ] ‚¨ú Telegram adapter implementation (5 days)
- [ ] ‚¨ú Migrate to TimescaleDB for metrics (3 days)
- [ ] ‚¨ú Add DeFi protocol data (2 days)
- [ ] ‚¨ú Implement circuit breaker integration (1 day)
- [ ] ‚¨ú Add automated proxy health tests (1 day)

---

## 11. Risk Register

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **IP ban from aggressive crawling** | Medium | High | Adaptive backoff, proxy rotation, politeness |
| **Free API quota exhaustion** | Medium | Medium | Monitor usage, implement fallbacks |
| **WebSocket connection instability** | High | Medium | Auto-reconnect, state recovery |
| **Database storage growth** | Low | Medium | TimescaleDB retention policies |
| **Third-party API deprecation** | Medium | High | Multiple sources per data type |
| **Proxy service downtime** | Low | Low | Fallback to direct connection |
| **Reddit API changes** | Low | Medium | Monitor PRAW updates, version pinning |

---

## 12. Final Recommendations

### ‚úÖ What to Keep

1. **Python-first architecture** - Excellent choice
2. **PostgreSQL + Redis** - Solid foundation
3. **Immutable raw data layer** - Critical for audit/replay
4. **Per-source rate limiting** - Essential for free tiers
5. **Adaptive backoff strategy** - Now implemented ‚úÖ

### üîß What to Improve

1. **Add WebSocket support with reconnection** (before enabling real-time)
2. **Integrate TimescaleDB** (when adding price/metrics)
3. **Implement monitoring dashboard** (Prometheus + Grafana)
4. **Add on-chain data sources** (critical blind spot)
5. **Setup proxy infrastructure** (3√ó VPS recommended)

### ‚ùå What to Avoid

1. **Don't use paid APIs** (per requirement) ‚úÖ
2. **Don't scrape Twitter directly** (use Nitter RSS instead)
3. **Don't use free public proxies** (use VPS proxies instead)
4. **Don't implement InfluxDB** (TimescaleDB sufficient)
5. **Don't over-engineer initially** (MVP first, scale later)

---

## 13. Conclusion

### Overall Grade: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)

**Strengths:**
- ‚úÖ Solid architectural foundation
- ‚úÖ Clear separation of concerns
- ‚úÖ Good immutability/audit practices
- ‚úÖ Conservative rate limiting approach
- ‚úÖ Adaptive improvements implemented

**Areas for Improvement:**
- ‚ö†Ô∏è Missing real-time data sources (WebSocket)
- ‚ö†Ô∏è No on-chain data integration
- ‚ö†Ô∏è Limited monitoring/observability
- ‚ö†Ô∏è Time-series database not yet integrated

**Verdict:**
D·ª± √°n Coin87 c√≥ n·ªÅn t·∫£ng k·ªπ thu·∫≠t **v·ªØng ch·∫Øc v√† kh·∫£ thi** cho m·ª•c ti√™u MVP. V·ªõi c√°c c·∫£i thi·ªán ƒë√£ implement (Reddit adapter, adaptive backoff, expanded sources) v√† roadmap r√µ r√†ng, h·ªá th·ªëng s·∫µn s√†ng cho giai ƒëo·∫°n development.

**Next Steps:**
1. Review v√† approve c√°c changes ƒë√£ implement
2. Install dependencies: `pip install -r backend/requirements.txt`
3. Configure Reddit API credentials trong `.env`
4. Test ingestion: `python backend/ingestion/jobs/run_ingestion.py`
5. Monitor results v√† iterate

---

**Document Version:** 1.0  
**Last Updated:** January 29, 2026  
**Reviewer:** Senior Data Engineer & Solution Architect  
**Status:** ‚úÖ Ready for Implementation
